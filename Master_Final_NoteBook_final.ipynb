{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1L-a_iGxo1aOoN7GSnbSEWmGK_jDTSXvB","timestamp":1703190007557}],"collapsed_sections":["tf-FjNVIaSEF"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Disclaimer:\n","This is the final master notebook, it is not meant to be able to run as a whole as all of us used our own drive links and ran them separately. The actual run of the individual steps are available in the other files in the git repo."],"metadata":{"id":"MrQrBIRpb-75"}},{"cell_type":"markdown","source":["# Data Preprocessing\n","\n","Before doing any actual training, we have to make sure the data is up to our standards to ensure the best possible accuracy."],"metadata":{"id":"55tu4iYdywE4"}},{"cell_type":"markdown","source":["## Enhanced labeling of real photos\n","[Link to GitHub where it showcases some runs of the codes below.](https://github.com/Frostforus/DTU_Deep_Learning_Image_Segmentation_Project/blob/farkas/process_real_imageset.ipynb)\n","\n","The real world data we were provided had several data quality issues. The biggest issue was that the masks were incorrect. We only found this out after looking through the initial numpy arrays that were provided. Using the images we used a clever way to retreive the masks from"],"metadata":{"id":"HIufEXMo1Izd"}},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","import os\n","import matplotlib.pyplot as plt\n","import colorsys\n","import scipy\n","\n","image_folder = '../carseg_data/images/photo/with_segmentation/'\n","array_folder = '../carseg_data/arrays/'\n","new_array_folder = 'data/new_arrays/'"],"metadata":{"id":"O0x6d9Ks60W6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Finding the right HSV thresholds\n","In order to correctly detect the colored areas on the jpg images we had to find the corresponding color thresholds. This was done by first visualizing histograms of the pixel values in the images. With the detected color values, we tested multiple color thresholds, by visualizing the results."],"metadata":{"id":"OiFV7QkF6q3Y"}},{"cell_type":"code","source":["# Creating a histogram of the hue values of a predefined image and showing the colours of the peak hue values on the histogram (to connect them to the parts)\n","\n","image_number = 1 # which image to use from the imageset\n","colors_to_show = 20 # how many colours to visualize based on the hue peaks\n","\n","im = cv2.imread(os.path.join(image_folder, str(image_number).zfill(4)+'.jpg'))\n","arr = np.load(os.path.join(array_folder, 'photo_'+str(image_number).zfill(4)+'.npy'))\n","\n","# makew the pixels labeled as background black\n","im[np.where(arr[...,-1]==0)] = [0, 0, 0]\n","\n","cv2.imshow('image', im)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()\n","\n","im = cv2.cvtColor(im, cv2.COLOR_BGR2HSV) # convert the image to hsv\n","\n","# plot the histogram with 100 bins\n","plt.hist(im[...,0].flatten()[im[...,0].flatten()!=0], bins = 100)\n","plt.show()\n","\n","# calculate the number of elements in each bin\n","hist, bin_edges = np.histogram(im[...,0].flatten()[im[...,0].flatten()!=0], bins=100)\n","\n","print('Peak H values:')\n","for i in range(1, colors_to_show+1):\n","\n","    # determine the i'th most common hue value based on the histogram\n","    h = round((bin_edges[hist.argsort()[-i]] + bin_edges[hist.argsort()[-i]+1]) / 2)\n","    print(h)\n","\n","    # plot a square of the color\n","    sample_color = (np.array(colorsys.hsv_to_rgb(h/179,1,1)) * 255).astype(int)\n","    print(sample_color)\n","    sample_im = np.empty((1,1,3))\n","    sample_im[...] = sample_color\n","    sample_im = sample_im.astype(int)\n","    plt.imshow(sample_im)\n","    plt.show()"],"metadata":{"id":"XS2TEx4a66Lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_number = 48 # which image to use from the imageset\n","\n","# use only the pixels at this hue value with this threshold\n","color_h = 60\n","color_h_threshold = 5\n","\n","im = cv2.imread(os.path.join(image_folder, str(image_number).zfill(4)+'.jpg'))\n","im_hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","\n","v_list = []\n","\n","# Collect the value parameters from the images when the hue is close to the predefined threshold\n","# This way we will be able to differentiate between light / dark green and pink / purple\n","for i in range(im.shape[0]):\n","    for j in range(im.shape[1]):\n","        if im_hsv[i,j,0] < color_h-color_h_threshold or im_hsv[i,j,0] > color_h+color_h_threshold:\n","            im[i,j,:] = [0,0,0]\n","        else:\n","            v_list.append(im_hsv[i,j,2])\n","\n","# Plot the values on a histogram\n","plt.hist(v_list, bins = 100)\n","plt.show()\n","\n","cv2.imshow('image', im)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"],"metadata":{"id":"-GVbmM5n7AMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_number = 1 # which image to use from the imageset\n","color_h = 60\n","color_h_threshold = 5\n","\n","im = cv2.imread(os.path.join(image_folder, str(image_number).zfill(4)+'.jpg'))\n","im_hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","\n","v_list = []\n","\n","# Collect the saturation parameters from the images when the hue is close to the predefined threshold\n","# This way we will be able to eliminate almost all false detection from the rest of the car\n","for i in range(im.shape[0]):\n","    for j in range(im.shape[1]):\n","        if im_hsv[i,j,0] < color_h-color_h_threshold or im_hsv[i,j,0] > color_h+color_h_threshold:\n","            im[i,j,:] = [0,0,0]\n","        else:\n","            v_list.append(im_hsv[i,j,1])\n","\n","# Plot the saturations on a histogram\n","plt.hist(v_list, bins = 100)\n","plt.show()\n","\n","cv2.imshow('image', im)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"],"metadata":{"id":"mYx_7eoU7D9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating the new masks\n","Based on the thresholds found by our visualizations we can recondtruct the correct masks from the jpg images. The detected areas below a defined pixel size were removed to get rid of false detection. Furthermore a morphological close was also applied to the masks to fill the few undetected pixels in the parts."],"metadata":{"id":"np1j5lBg59DG"}},{"cell_type":"code","source":["image_list = os.listdir(image_folder)\n","\n","# minimal contiguous area to be detected as a part\n","min_mask_area = 40\n","\n","# go though all the jpg images\n","for image_name in image_list:\n","    im = cv2.imread(os.path.join(image_folder, image_name))\n","\n","    image_number = int(image_name[:-4])\n","    arr = np.load(os.path.join(array_folder, 'photo_'+str(image_number).zfill(4)+'.npy'))\n","\n","    im = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","\n","    # define the new array where we will store the results of our algorithm\n","    new_arr = arr.copy()\n","    new_arr[...,-1].fill(0)\n","\n","    categories = [10,20,30,40,50,60,70,80]\n","\n","    # create a separate mask for each category\n","    masks = np.zeros((len(categories), arr.shape[0], arr.shape[1]))\n","\n","    # go through every pixel one by one\n","    for i in range(arr.shape[0]):\n","        for j in range(arr.shape[1]):\n","\n","            # if the pixel is not labeled as background\n","            if arr[i,j,-1] != 0:\n","\n","                # create a label in one of the masks based on the colour of the pixel\n","                # if neither of these are true the pixel will be categorized as rest of the car\n","                if im[i,j,0] >= 10 and im[i,j,0] <= 20 and im[i,j,1] >= 180 and im[i,j,2] >= 100:\n","                    masks[0,i,j] = 1\n","                if im[i,j,0] >= 55 and im[i,j,0] <= 65 and im[i,j,1] >= 180 and im[i,j,2] >= 70 and im[i,j,2] <= 130:\n","                    masks[1,i,j] = 1\n","                if im[i,j,0] >= 25 and im[i,j,0] <= 35 and im[i,j,1] >= 180 and im[i,j,2] >= 100:\n","                    masks[2,i,j] = 1\n","                if im[i,j,0] >= 85 and im[i,j,0] <= 95 and im[i,j,1] >= 180 and im[i,j,2] >= 100:\n","                    masks[3,i,j] = 1\n","                if im[i,j,0] >= 145 and im[i,j,0] <= 155 and im[i,j,1] >= 180 and im[i,j,2] >= 100 and im[i,j,2] <= 180:\n","                    masks[4,i,j] = 1\n","                if im[i,j,0] >= 55 and im[i,j,0] <= 65 and im[i,j,1] >= 180 and im[i,j,2] >= 130:\n","                    masks[5,i,j] = 1\n","                if im[i,j,0] >= 115 and im[i,j,0] <= 125 and im[i,j,1] >= 180 and im[i,j,2] >= 100:\n","                    masks[6,i,j] = 1\n","                if im[i,j,0] >= 145 and im[i,j,0] <= 155 and im[i,j,1] >= 180 and im[i,j,2] > 180:\n","                    masks[7,i,j] = 1\n","\n","    # go through each created mask\n","    for c_n, c in enumerate(categories):\n","\n","        # count the contiguous area of the true areas in the current mask\n","        # in mask_areas every pixel will have the 0 or the size of the true area they are part of as their value\n","        mask_labeled, labels_n = scipy.ndimage.label(masks[c_n])\n","        mask_areas = np.zeros(masks[c_n].shape)\n","        for l in range(1,labels_n+1):\n","            mask_areas[mask_labeled==l] = np.sum(mask_labeled==l)\n","        masks[c_n,...][mask_areas<min_mask_area] = 0 # remove the areas smaller than the predefined size\n","\n","        # apply a morphological close with a 5x5 circular kernel to fill the undetected pixels in bigger areas\n","        masks[c_n] = cv2.morphologyEx(masks[c_n], cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5)))\n","\n","        # change the category labels to match the ones in the dataset from Deloitte\n","        for i in range(arr.shape[0]):\n","            for j in range(arr.shape[1]):\n","                if masks[c_n,i,j]:\n","                    new_arr[i,j,-1] = c\n","\n","    # add the labels for the rest of the car to the array\n","    for i in range(arr.shape[0]):\n","        for j in range(arr.shape[1]):\n","            if arr[i,j,-1] != 0 and new_arr[i,j,-1] == 0:\n","                new_arr[i,j,-1] = 90\n","\n","    # save the newly created array\n","    np.save(os.path.join(new_array_folder, 'photo_'+str(image_number).zfill(4)+'.npy'), new_arr)"],"metadata":{"id":"mvXouk7D7G55"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Augmentation\n","\n","[Link to Google colab where it showcases some runs.](https://colab.research.google.com/drive/1fslwnPv_Lq_hmmiZrmQg6VYrtTGMAyCM?usp=sharing)\n","\n"],"metadata":{"id":"Js8uHLgI1LAD"}},{"cell_type":"markdown","source":["Since the test images were real images, it was essential to include as many real car images in the test dataset as possible. Only 169 real images were provided (30 out of 169 were test images), thus we augmented the real images. We applied:\n","*   Rotation\n","*   Vertical\n","*   Dropout\n","*   Gaussin blur\n","*   Sharpening\n","\n","\n","\n"],"metadata":{"id":"bl4gKH2ii4r3"}},{"cell_type":"markdown","source":["### Function: apply image augmentation\n","This function takes the original image as an input, apply different augmentation methods on it and gives the transfered image back as an output"],"metadata":{"id":"-Ti2PVObhyyS"}},{"cell_type":"code","source":["import imgaug as ia\n","from imgaug import augmenters as iaa\n","import cv2\n","import json\n","import tensorflow as tf\n","from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n","import imageio\n","import imgaug.imgaug\n","import time\n","\n","####################################################\n","# AUGMENTATION\n","####################################################\n","\n","# Set a seed based on the current time\n","np.random.seed(int(time.time()))\n","\n","def augment_numpy_arrays(input_directory, file_names, output_directory):\n","    # Iterate through the file names\n","    for filename in file_names:\n","        # ia.seed(1)\n","\n","        # Load the numpy array from the file path\n","        array = np.load(input_directory + filename)\n","\n","        image = array[...,:3]\n","        segmap = array[...,-1].reshape((256, 256, 1))\n","\n","        # Apply dropout\n","        dropout_rate = np.random.uniform(0.05, 0.08)\n","        augmented_rgb = iaa.Dropout([dropout_rate])(image=image)\n","        augmented_mask = segmap\n","\n","        # Apply Gaussian (probability:80%)\n","        ugmented_rgb = iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 2)))(images=augmented_rgb)\n","\n","        # # Apply sharpening\n","        augmented_rgb = iaa.Sharpen((0.0, 0.1))(image=image)\n","\n","        # Apply rotation\n","        rotation_angle = 20 #np.random.uniform(-20, 20)\n","        augmented_rgb, augmented_mask = iaa.Affine(rotate=rotation_angle, order=0, mode=\"constant\", cval=0)(images=[augmented_rgb,segmap])\n","        # augmented_rgb, augmented_mask = iaa.Affine(rotate=rotation_angle, order=0, mode=\"constant\", cval=0)(images=[image,segmap])\n","\n","        # # Apply elastic transformation\n","        array_rgb = iaa.ElasticTransformation(alpha=20, sigma=5)(image=array_rgb)\n","        array_gray = iaa.ElasticTransformation(alpha=20, sigma=5)(image=array_gray)\n","\n","        # Apply horizontal flip\n","        augmented_rgb, augmented_mask = iaa.Fliplr(1)(images=[augmented_rgb, augmented_mask])\n","        # augmented_rgb, augmented_mask = iaa.Fliplr(1)(images=[image,segmap])\n","\n","        # Apply brightness multiplication\n","        augmented_rgb =  iaa.Multiply((0.8, 1.1))(image=augmented_rgb)\n","\n","        # Concatenate the RGB and mask arrays\n","        combined_array = np.concatenate([augmented_rgb, augmented_mask], axis=-1)\n","\n","        # Save the array\n","        np.save(output_directory + \"augmented8_\" + filename, combined_array)\n","\n","    print(\"Augmentation is done.\")"],"metadata":{"id":"szopEL3GzEWK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Input and output folders\n","\n","In this section the input and output forders are defined for data augmentation. We take the arrays from the input folder, call the previously created augmentation function on them and save the results into the output folder.\n","\n","Here the input folder is the improved arrays of the real car images"],"metadata":{"id":"OrzSeKF2m_nZ"}},{"cell_type":"code","source":["aug_input_path = '/content/drive/MyDrive/Saját/DTU/semester-3/Deep_Learning/DTU_Deep_Learning_Image_Segmentation_Project/data/new_arrays/'\n","aug_output_path = '/content/drive/MyDrive/Saját/DTU/semester-3/Deep_Learning/DTU_Deep_Learning_Image_Segmentation_Project/data/augmented_arrays_rot20_flip/'\n","augment_numpy_arrays(aug_input_path, black_car_image_file_names, aug_output_path)"],"metadata":{"id":"EaKqk_EF1Rhq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Adding background images to CAD images.\n","[Link to Google colab where it showcases some runs.](https://colab.research.google.com/drive/1EDB2RnDnX4fKyJiQDGjDHAg4gL0Xmnlj?usp=sharing)\n","\n","In this section we add more realistic background images to the given CAD car images, so the model is given the chance to learn that the background can be different every time."],"metadata":{"id":"bEw5TyfyZGZb"}},{"cell_type":"code","source":["import os\n","import re\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def filter_files_by_regex(directory, pattern):\n","    # Use os.listdir() to get a list of all files and directories in the specified directory\n","    file_list = os.listdir(directory)\n","    # Use a list comprehension to filter the files based on the regex pattern\n","    filtered_files = [directory+filename for filename in file_list if re.match(pattern, filename)]\n","\n","    return filtered_files"],"metadata":{"id":"RfzRYUx4ZrEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Function: load car onto given background\n","This function takes the background image and puts the car onto it, shifted down a bit so it isn't flying. We know where the car is using both the masks given and the absolute value of the pixels. Sadly some of the training data given for the orange cars were unusable because of the masks provided, we didn't run this on such pictures."],"metadata":{"id":"tf-FjNVIaSEF"}},{"cell_type":"code","source":["def load_car_onto_background(car_path,background_path):\n","    original_background = Image.open(background_path)\n","\n","    # Resize background\n","    resized_image = original_background.resize((256, 256))\n","\n","    # Convert the PIL Image to a NumPy array\n","    image_array2 = np.array(resized_image).astype(np.uint8)\n","    image_array = np.zeros((256, 256, 4)).astype(np.uint8)\n","\n","    image_array[...,:3] = image_array2\n","\n","    #get car image from npy file\n","    car_array = np.load(car_path)\n","\n","\n","    black_pixel = np.array([0,0,0])\n","    for i in range(256):\n","            for j in range(256):\n","              if  not (np.array_equal(car_array[i,j,3], 0)):\n","                if i+128 > 255 or j >255: continue\n","                image_array[i+128,j] = car_array[i,j]\n","\n","\n","    plt.imshow(image_array2)  # Use 'image_array_uint8' if values are in [0, 255]\n","    plt.title('Background Image')\n","    plt.axis('off')  # Optional: Turn off axis labels\n","    plt.show()\n","\n","    plt.imshow(car_array[...,:3])  # Use 'image_array_uint8' if values are in [0, 255]\n","    plt.title('Car Image')\n","    plt.axis('off')  # Optional: Turn off axis labels\n","    plt.show()\n","\n","    plt.imshow(image_array[...,:3])  # Use 'image_array_uint8' if values are in [0, 255]\n","    plt.title('Car image with background')\n","    plt.axis('off')  # Optional: Turn off axis labels\n","    plt.show()\n","    return image_array"],"metadata":{"id":"1Pqh3RbnZrA4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save images to drive\n","In this block we save the images to the drive, where it can be moved into the training dataset."],"metadata":{"id":"JNF-IFB-Z9Hk"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1giWQKiGZq-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory_path = 'drive/My Drive/___Deeplearning/backgrounds_roads/'\n","# regex_pattern = r'black_5_doors_[0-9]*\\.npy'\n","regex_pattern = r'[0-9]*\\.jpg'\n","background_file_names = sorted(filter_files_by_regex(directory_path, regex_pattern))"],"metadata":{"id":"plc-8sphZq74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory_path = 'drive/My Drive/___Deeplearning/arrays/'\n","# regex_pattern = r'black_5_doors_[0-9]*\\.npy'\n","regex_pattern = r'black_5_doors_[0-9]*\\.npy'\n","photos_nparray_file_names = sorted(filter_files_by_regex(directory_path, regex_pattern))"],"metadata":{"id":"epTC0RaGZuv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory_path = 'drive/My Drive/___Deeplearning/arrays/'\n","#regex_pattern = r'black_5_doors_[0-9]*\\.npy'\n","regex_pattern = r'orange_3_doors_[0-9]*\\.npy'\n","orange_nparray_file_names = sorted(filter_files_by_regex(directory_path, regex_pattern))"],"metadata":{"id":"OGvVmxcFZusi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_images_with_backgrounds(car_file_paths, background_file_paths,out_path='drive/My Drive/___Deeplearning/arrays/cars_with_backgrounds/',file_name=\"Black_with_background\", max_load=10,shift =0, step=1):\n","  for i in range(shift,shift+len(car_file_paths)):\n","    if i >= max_load+shift: break\n","    #print(i)\n","    car_with_background = load_car_onto_background(car_file_paths[i*step],background_file_paths[i*step % len(background_file_paths)])\n","\n","    np.save(out_path+f\"{file_name}_{i}.npy\",car_with_background)\n","    break\n"],"metadata":{"id":"HWhdtWgrZxHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_images_with_backgrounds(orange_nparray_file_names,background_file_names,file_name=\"Black_with_background\",max_load=1000)"],"metadata":{"id":"9LVllsBOZxEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# UNET Definition\n","\n","We used a unet model similar to the one used by the semantic shapes git repo, mentioned in our references. This provided good performance from the start. The Unet itself is a typica unet archicture, using maxpooling to shrik the dimensions in the encoding layers, and using skip layers (concat layers) and conv2dtranspose layers in the decode layers. The architecture itself helps keep locational information as well as being able to learn abstract features, while keeping fine grained details."],"metadata":{"id":"Bf1wsBRn07-I"}},{"cell_type":"code","source":["from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, concatenate, Dropout,\\\n","                                    Lambda, Conv2DTranspose, Add\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","import tensorflow as tf\n","import os\n","\n","\n","imshape = (256, 256, 3)\n","n_classes = len(data)\n","\n","\n","def preprocess_input(x):\n","    x /= 255.\n","    x -= 0.5\n","    x *= 2.\n","    return x\n","\n","\n","def dice(y_true, y_pred, smooth=1.):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","def unet(pretrained=False, base=4):\n","\n","    if pretrained:\n","        path = os.path.join('models', model_name+'.model')\n","        if os.path.exists(path):\n","            model = load_model(path, custom_objects={'dice': dice})\n","            model.summary()\n","            return model\n","        else:\n","            print('Failed to load existing model at: {}'.format(path))\n","\n","    if n_classes == 1:\n","        loss = 'binary_crossentropy'\n","        final_act = 'sigmoid'\n","    elif n_classes > 1:\n","        loss = 'categorical_crossentropy'\n","        final_act = 'softmax'\n","\n","    b = base\n","    i = Input((imshape[0], imshape[1], imshape[2]))\n","    s = Lambda(lambda x: preprocess_input(x)) (i)\n","\n","    c1 = Conv2D(2**b, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n","    c1 = Dropout(0.1) (c1)\n","    c1 = Conv2D(2**b, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n","    p1 = MaxPooling2D((2, 2)) (c1)\n","\n","    c2 = Conv2D(2**(b+1), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n","    c2 = Dropout(0.1) (c2)\n","    c2 = Conv2D(2**(b+1), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n","    p2 = MaxPooling2D((2, 2)) (c2)\n","\n","    c3 = Conv2D(2**(b+2), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n","    c3 = Dropout(0.2) (c3)\n","    c3 = Conv2D(2**(b+2), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n","    p3 = MaxPooling2D((2, 2)) (c3)\n","\n","    c4 = Conv2D(2**(b+3), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n","    c4 = Dropout(0.2) (c4)\n","    c4 = Conv2D(2**(b+3), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n","    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n","\n","    c5 = Conv2D(2**(b+4), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n","    c5 = Dropout(0.3) (c5)\n","    c5 = Conv2D(2**(b+4), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n","\n","    u6 = Conv2DTranspose(2**(b+3), (2, 2), strides=(2, 2), padding='same') (c5)\n","    u6 = concatenate([u6, c4])\n","    c6 = Conv2D(2**(b+3), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n","    c6 = Dropout(0.2) (c6)\n","    c6 = Conv2D(2**(b+3), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n","\n","    u7 = Conv2DTranspose(2**(b+2), (2, 2), strides=(2, 2), padding='same') (c6)\n","    u7 = concatenate([u7, c3])\n","    c7 = Conv2D(2**(b+2), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n","    c7 = Dropout(0.2) (c7)\n","    c7 = Conv2D(2**(b+2), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n","\n","    u8 = Conv2DTranspose(2**(b+1), (2, 2), strides=(2, 2), padding='same') (c7)\n","    u8 = concatenate([u8, c2])\n","    c8 = Conv2D(2**(b+1), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n","    c8 = Dropout(0.1) (c8)\n","    c8 = Conv2D(2**(b+1), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n","\n","    u9 = Conv2DTranspose(2**b, (2, 2), strides=(2, 2), padding='same') (c8)\n","    u9 = concatenate([u9, c1], axis=3)\n","    c9 = Conv2D(2**b, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n","    c9 = Dropout(0.1) (c9)\n","    c9 = Conv2D(2**b, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n","\n","    o = Conv2D(n_classes, (1, 1), activation=final_act) (c9)\n","\n","    model = Model(inputs=i, outputs=o)\n","    model.compile(optimizer=Adam(1e-4),\n","                  loss=loss,\n","                  metrics=[dice])\n","    model.summary()\n","\n","    return model"],"metadata":{"id":"0hTvZGut1Q0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameter Optimization\n","\n","We ran the hyperparameter optimization for different optimizers, batch sizes, learning rates and losses to see which combination gives us the best results. For that sake, we also defined different loss functions below.\n","[Link to Google colab where it showcases some runs.](https://colab.research.google.com/drive/1Bnk7cO6686d2v3Ph995R3QLQyAkDe2a0?usp=sharing)"],"metadata":{"id":"V1Jv48Cv1AvP"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n","\n","def binary_crossentropy_loss(y_true, y_pred):\n","    return binary_crossentropy(y_true, y_pred)\n","\n","def categorical_cross_entropy_loss(y_true, y_pred):\n","    return categorical_crossentropy(y_true, y_pred)\n","\n","def DiceLoss(targets, inputs, smooth=1e-6):\n","    inputs = K.flatten(inputs)\n","    targets = K.flatten(targets)\n","    intersection = K.sum(targets * inputs)\n","    return 1-(2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","\n","def DiceBCELoss(targets, inputs, smooth=1e-6):\n","\n","    # Calculate BCE loss\n","    BCE = binary_crossentropy_loss(targets, inputs)\n","\n","    # Calculate intersection and dice loss\n","    dice_loss = DiceLoss(targets, inputs)\n","\n","    # Combine BCE and dice loss\n","    Dice_BCE = BCE + dice_loss\n","\n","    return Dice_BCE"],"metadata":{"id":"A9QBz3sqbwPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import ParameterGrid\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","\n","\n","# Split the data into training and validation sets\n","def sorted_fns(dir):\n","    return sorted(os.listdir(dir), key=lambda x: int(x.split('.')[0][-4:]))\n","\n","all_paths = [os.path.join(directory_path, x) for x in sorted_fns(directory_path)]\n","\n","# Split the data into training and validation sets\n","train_paths, val_paths = train_test_split(all_paths, test_size=0.2, random_state=42)\n","\n","# Define hyperparameter grid\n","param_grid = {\n","    'optimizer': [Adam, RMSprop, SGD],  # Use optimizer classes, not strings (RMSprop)\n","    'learning_rate': [1e-5, 1e-3, 1e-4], #1e-2,\n","    'batch_size': [8, 16, 32],\n","    'loss': [DiceLoss, categorical_cross_entropy_loss, DiceBCELoss] #binary_crossentropy_loss,DiceLoss\n","}\n","\n","# Generate all combinations of hyperparameters\n","param_combinations = list(ParameterGrid(param_grid))\n","\n","# Perform grid search\n","best_params = None\n","best_loss = float('inf')\n","best_model_number = None\n","model_counter = 1\n","\n","for params in param_combinations:\n","\n","    print(f\"\\nTraining model with parameters: {params}\")\n","\n","    model = unet(pretrained=False, base=4)\n","\n","    tg = DataGenerator(paths=train_paths, batch_size=params['batch_size'], augment=True)\n","\n","    model.compile(optimizer=params['optimizer'](learning_rate=params['learning_rate']),\n","                  loss=params['loss'],\n","                  metrics=[dice])\n","\n","    # Define EarlyStopping callback\n","    early_stopping = EarlyStopping(monitor='val_loss',min_delta=0.005,mode=\"min\", patience=5, restore_best_weights=True)\n","\n","    # Define validation generator\n","    val_generator = DataGenerator(paths=val_paths, batch_size=params['batch_size'], augment=False)\n","\n","    model.fit_generator(generator=tg,\n","                        steps_per_epoch=len(tg),\n","                        epochs=100,  # Adjust the number of epochs based on your needs\n","                        validation_data=val_generator,\n","                        validation_steps=len(val_generator),\n","                        verbose=1,\n","                        callbacks=[early_stopping])\n","\n","    # Evaluate on the validation set\n","    val_loss = model.evaluate_generator(generator=val_generator, steps=len(val_generator))\n","\n","    if val_loss[0] < best_loss:\n","        best_loss = val_loss[0]\n","        best_params = params\n","        best_model_number = model_counter\n","\n","    model_counter += 1\n","\n","# Print the best hyperparameters and the corresponding model number\n","print(f\"Best Hyperparameters for Model {best_model_number}:\", best_params)"],"metadata":{"id":"JPYhD2Td1QXW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final Training\n","In the final training we trained our model with the best parameters that we received in the hyperparameter optimization. We set 200 epochs to be sure we give enough iterations for the model to achieve proper results however, with early stopping already 130 was enough.\n","[Link to Google colab where it showcases some runs.](https://colab.research.google.com/drive/1tyVwgMh-6IZ4REJ_7dydBFzHy22eCt_0?usp=sharing)"],"metadata":{"id":"PwM8_LOq1dw0"}},{"cell_type":"code","source":["import os\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","\n","\n","\n","def sorted_fns(dir):\n","    return sorted(os.listdir(dir), key=lambda x: int(x.split('.')[0][-4:]))\n","\n","\n","\n","all_paths = [os.path.join(directory_path, x) for x in sorted_fns(directory_path)]\n","print(all_paths[1])\n","\n","# Split the data into training and validation sets\n","train_paths, val_paths = train_test_split(all_paths, test_size=0.2)#, random_state=42)\n","print(len(train_paths))\n","print(len(val_paths))\n","print(train_paths[0])\n","\n","model = unet(pretrained=False, base=4)\n","\n","tg = DataGenerator(paths=train_paths, batch_size=32, augment=False)\n","\n","# Define EarlyStopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', mode=\"min\", verbose=1, patience=30, min_delta=0.005)#, restore_best_weights=True)\n","\n","# # Define validation generator\n","val_generator = DataGenerator(paths=val_paths, batch_size=32, augment=False)"],"metadata":{"id":"w147JGAnKShr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(tg,\n","          steps_per_epoch=len(tg),\n","          epochs=200,\n","          verbose=1,\n","          validation_data=val_generator,\n","          # validation_steps=len(val_generator)\n","          callbacks=[model_checkpoint_callback, early_stopping]#[early_stopping, checkpoint, train_val, tb_mask]\n","          )"],"metadata":{"id":"NaVDi5fuKZ-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Saving out the model\n","After the training we saved out the model to use it later on for the results"],"metadata":{"id":"exj0cCzkYsjP"}},{"cell_type":"code","source":["savemodelto = 'drive/My Drive/carseg_data/'\n","model.save(savemodelto+'model.keras')"],"metadata":{"id":"wMMtfJQRZN4s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install h5py"],"metadata":{"id":"Gv1zpHOeZRrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import model_from_json\n","\n","# serialize model to JSON\n","model_json = model.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","\n","model.save_weights(\"model.h5\")"],"metadata":{"id":"nyan-idHZWPm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Model saved.\")"],"metadata":{"id":"TrY2ifyFZY_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results with dice values\n","After we trained the model we represented the achieved results using the three different dice values. Also to get a feeling regarding the overall performance with all the test images, we took the averages of the dice values.\n","[Link to Google colab where it showcases some runs.](https://colab.research.google.com/drive/1tyVwgMh-6IZ4REJ_7dydBFzHy22eCt_0?usp=sharing)"],"metadata":{"id":"l2greFxv0-_n"}},{"cell_type":"code","source":["class_to_rgb = {\n","    1: [255,128,0],\n","    2: [0,255,0],\n","    3: [255,255,0],\n","    4: [0,255,255],\n","    5: [128,0,255],\n","    6: [0,255,128],\n","    7: [0,128,255],\n","    8: [255,0,128],\n","    9: [255,255,255],\n","    0: [0,0,0],\n","}"],"metadata":{"id":"AXz1k-lE0zAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMh2bzZByr2X"},"outputs":[],"source":["def classToRGB(_paddedPrediction,_class_to_rgb):\n","  for x in range(0,_paddedPrediction.shape[0]):\n","    for y in range(0,_paddedPrediction.shape[1]):\n","      _paddedPrediction[x][y] = _class_to_rgb[_paddedPrediction[x][y][0]]"]},{"cell_type":"code","source":["directory_path = 'drive/My Drive/carseg_data/test/'\n","# regex_pattern = r'black_5_doors_[0-9]*\\.npy'\n","regex_pattern = r'photo_[0-9]*\\.npy'\n","black_car_image_file_names = sorted(filter_files_by_regex(directory_path, regex_pattern))"],"metadata":{"id":"qpMG8XvxKkK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numpy_arraysx, numpy_arraysy = load_numpy_arrays(directory_path, black_car_image_file_names, max_load=30)"],"metadata":{"id":"UZ4OHKq7Km40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_weights(checkpoint_filepath)"],"metadata":{"id":"cGhWA3vsKo4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize variables to accumulate dice values\n","total_full_dice = 0\n","total_car_dice = 0\n","total_parts_dice = 0\n","\n","# Predict all images loaded in\n","for i in range(0, len(numpy_arraysx)):\n","    y_true = np.load(f'drive/My Drive/carseg_data/new_arrays/photo_{str(i+1).zfill(4)}.npy')\n","    y_true = (y_true[...,-1] / 10).astype(int)\n","\n","    # Image to predict\n","    image = numpy_arraysx[i].reshape(1, 256, 256, 3)\n","    prediction = np.argmax(model.predict(image, verbose=0).squeeze(), axis=2)\n","\n","    car = y_true != 0\n","    car_part = np.logical_and(y_true != 9, car)\n","\n","    full_dice = 2 * (prediction == y_true).sum() / 256 / 256 / 2\n","    car_dice = 2 * (np.logical_and(prediction == y_true, car)).sum() / car.sum() / 2\n","    parts_dice = 2 * (np.logical_and(prediction == y_true, car_part)).sum() / car_part.sum() / 2\n","\n","    # Accumulate dice values\n","    total_full_dice += full_dice\n","    total_car_dice += car_dice\n","    total_parts_dice += parts_dice\n","\n","    print(f\"Image {i+1}:\")\n","    print(\"Full dice:\", full_dice)\n","    print(\"Car dice:\", car_dice)\n","    print(\"Parts dice:\", parts_dice)\n","\n","    paddedPrediction = np.pad(prediction[...,np.newaxis], ((0,0),(0,0),(0,2)), mode='constant', constant_values=1)\n","    classToRGB(paddedPrediction, class_to_rgb)\n","    # Predictions:\n","    # Input and expected output:\n","    show_image(numpy_arraysx[i])\n","    show_image(numpy_arraysy[i])\n","    show_image(paddedPrediction)\n","\n","# Calculate averages\n","avg_full_dice = total_full_dice / len(numpy_arraysx)\n","avg_car_dice = total_car_dice / len(numpy_arraysx)\n","avg_parts_dice = total_parts_dice / len(numpy_arraysx)\n","\n","# Print averages\n","print(\"\\nAverage Dice Values:\")\n","print(\"Average Full Dice:\", avg_full_dice)\n","print(\"Average Car Dice:\", avg_car_dice)\n","print(\"Average Parts Dice:\", avg_parts_dice)"],"metadata":{"id":"cLyQUuQoKqdn"},"execution_count":null,"outputs":[]}]}